#!/usr/bin/env python3
"""
æ•°æ®é©±åŠ¨åˆ†æå™¨ - åŸºäºçœŸå®SQLæ‰§è¡Œç»“æœç”Ÿæˆåˆ†ææŠ¥å‘Š
Data-Driven Analyzer - Generate analysis reports based on actual SQL execution results
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class DataDrivenAnalyzer:
    """åŸºäºçœŸå®æ•°æ®çš„åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.analysis_keywords = [
            'åˆ†æ', 'æŠ¥å‘Š', 'æ€»ç»“', 'æ ¹å› ', 'åŸå› åˆ†æ',
            'analysis', 'analyze', 'report', 'summary', 'root cause'
        ]
    
    def should_generate_analysis_report(self, user_input: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not user_input:
            return False
        
        user_input_lower = user_input.lower()
        return any(keyword.lower() in user_input_lower for keyword in self.analysis_keywords)
    
    def generate_data_driven_report(self, 
                                  result_df: pd.DataFrame, 
                                  user_input: str, 
                                  sql: str) -> Dict[str, Any]:
        """åŸºäºçœŸå®æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        
        try:
            if result_df.empty:
                return self._generate_empty_data_report(user_input, sql)
            
            # åˆ†ææ•°æ®ç‰¹å¾
            data_insights = self._analyze_data_characteristics(result_df)
            
            # ç”Ÿæˆä¸šåŠ¡åˆ†æ
            business_analysis = self._generate_business_analysis(result_df, user_input, data_insights)
            
            # æ„å»ºå®Œæ•´æŠ¥å‘Š
            report = {
                "summary": business_analysis["summary"],
                "key_findings": business_analysis["key_findings"],
                "insights": business_analysis["insights"],
                "recommendations": business_analysis["recommendations"],
                "methodology": business_analysis["methodology"]
            }
            
            logger.info(f"Generated data-driven analysis report with {len(result_df)} records")
            return report
            
        except Exception as e:
            logger.error(f"Error generating data-driven report: {str(e)}")
            return self._generate_fallback_report(user_input, sql, str(e))
    
    def _analyze_data_characteristics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç‰¹å¾"""
        insights = {
            "record_count": len(df),
            "column_count": len(df.columns),
            "numeric_columns": [],
            "date_columns": [],
            "categorical_columns": [],
            "trends": {},
            "statistics": {},
            "patterns": {}
        }
        
        # åˆ†æåˆ—ç±»å‹
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                insights["numeric_columns"].append(col)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                insights["statistics"][col] = {
                    "mean": df[col].mean(),
                    "median": df[col].median(),
                    "std": df[col].std(),
                    "min": df[col].min(),
                    "max": df[col].max(),
                    "null_count": df[col].isnull().sum()
                }
                
                # åˆ†æè¶‹åŠ¿ï¼ˆå¦‚æœæœ‰å¤šè¡Œæ•°æ®ï¼‰
                if len(df) > 1:
                    first_val = df[col].iloc[0]
                    last_val = df[col].iloc[-1]
                    if pd.notnull(first_val) and pd.notnull(last_val):
                        change_rate = (last_val - first_val) / first_val if first_val != 0 else 0
                        insights["trends"][col] = {
                            "direction": "ä¸Šå‡" if change_rate > 0.05 else "ä¸‹é™" if change_rate < -0.05 else "ç¨³å®š",
                            "change_rate": change_rate,
                            "first_value": first_val,
                            "last_value": last_val
                        }
            
            elif 'date' in col.lower() or 'time' in col.lower():
                insights["date_columns"].append(col)
            else:
                insights["categorical_columns"].append(col)
        
        # è¯†åˆ«ç‰¹æ®Šæ¨¡å¼
        insights["patterns"] = self._identify_patterns(df, insights)
        
        return insights
    
    def _identify_patterns(self, df: pd.DataFrame, insights: Dict) -> Dict[str, Any]:
        """è¯†åˆ«æ•°æ®æ¨¡å¼"""
        patterns = {}
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é€¾æœŸç‡åˆ†æ
        mob_columns = [col for col in df.columns if 'MOB' in str(col).upper()]
        if mob_columns:
            patterns["analysis_type"] = "é€¾æœŸç‡åˆ†æ"
            patterns["mob_periods"] = mob_columns
            
            # åˆ†æMOBæœŸæ•°çš„é€¾æœŸç‡å˜åŒ–
            if len(mob_columns) > 1:
                mob_trends = []
                for col in mob_columns:
                    if col in insights["statistics"]:
                        avg_rate = insights["statistics"][col]["mean"]
                        mob_trends.append((col, avg_rate))
                
                patterns["mob_trend_analysis"] = mob_trends
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´åºåˆ—åˆ†æ
        date_cols = insights["date_columns"] + [col for col in df.columns if any(x in col.lower() for x in ['æœˆä»½', 'month', 'æ—¥æœŸ', 'date'])]
        if date_cols:
            patterns["analysis_type"] = "æ—¶é—´åºåˆ—åˆ†æ"
            patterns["time_columns"] = date_cols
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        total_cells = len(df) * len(df.columns)
        null_cells = df.isnull().sum().sum()
        patterns["data_quality"] = {
            "completeness": (total_cells - null_cells) / total_cells,
            "null_percentage": null_cells / total_cells
        }
        
        return patterns
    
    def _generate_business_analysis(self, df: pd.DataFrame, user_input: str, data_insights: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆä¸šåŠ¡åˆ†æ"""
        
        analysis_type = data_insights["patterns"].get("analysis_type", "é€šç”¨æ•°æ®åˆ†æ")
        
        if analysis_type == "é€¾æœŸç‡åˆ†æ":
            return self._generate_overdue_analysis(df, user_input, data_insights)
        elif analysis_type == "æ—¶é—´åºåˆ—åˆ†æ":
            return self._generate_time_series_analysis(df, user_input, data_insights)
        else:
            return self._generate_general_analysis(df, user_input, data_insights)
    
    def _generate_overdue_analysis(self, df: pd.DataFrame, user_input: str, data_insights: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆé€¾æœŸç‡åˆ†ææŠ¥å‘Š"""
        
        mob_columns = data_insights["patterns"].get("mob_periods", [])
        statistics = data_insights["statistics"]
        trends = data_insights["trends"]
        
        # ç”Ÿæˆæ‘˜è¦
        avg_rates = []
        for col in mob_columns:
            if col in statistics:
                avg_rate = statistics[col]["mean"]
                avg_rates.append(f"{col}: {avg_rate:.2%}")
        
        summary = f"åŸºäº{len(df)}æ¡è®°å½•çš„é€¾æœŸç‡åˆ†ææ˜¾ç¤ºï¼Œ{', '.join(avg_rates[:3])}ã€‚æ•°æ®è¦†ç›–{len(mob_columns)}ä¸ªMOBæœŸæ•°ï¼Œä¸ºé£é™©ç®¡ç†æä¾›äº†è¯¦ç»†çš„é‡åŒ–ä¾æ®ã€‚"
        
        # ç”Ÿæˆå…³é”®å‘ç°
        key_findings = []
        key_findings.append(f"ğŸ” å…±åˆ†æ{len(df)}æ¡è®°å½•ï¼Œæ¶µç›–{len(mob_columns)}ä¸ªMOBæœŸæ•°çš„é€¾æœŸè¡¨ç°")
        
        # åˆ†æå…·ä½“æ•°å€¼
        for col in mob_columns[:3]:  # åªåˆ†æå‰3ä¸ªMOBæœŸ
            if col in statistics:
                stats = statistics[col]
                key_findings.append(f"ğŸ” {col}å¹³å‡é€¾æœŸç‡ä¸º{stats['mean']:.2%}ï¼Œæœ€é«˜{stats['max']:.2%}ï¼Œæœ€ä½{stats['min']:.2%}")
        
        # åˆ†æè¶‹åŠ¿
        for col in mob_columns:
            if col in trends:
                trend = trends[col]
                key_findings.append(f"ğŸ” {col}æœŸæ•°é€¾æœŸç‡å‘ˆ{trend['direction']}è¶‹åŠ¿ï¼Œå˜åŒ–å¹…åº¦{trend['change_rate']:.1%}")
        
        # æ•°æ®è´¨é‡åˆ†æ
        data_quality = data_insights["patterns"]["data_quality"]
        key_findings.append(f"ğŸ” æ•°æ®å®Œæ•´åº¦{data_quality['completeness']:.1%}ï¼Œæ•°æ®è´¨é‡{'è‰¯å¥½' if data_quality['completeness'] > 0.9 else 'éœ€è¦æ”¹å–„'}")
        
        # ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
        insights = []
        
        # åŸºäºå®é™…æ•°å€¼çš„æ´å¯Ÿ
        high_risk_periods = []
        for col in mob_columns:
            if col in statistics and statistics[col]["mean"] > 0.1:  # 10%ä»¥ä¸Šè®¤ä¸ºé«˜é£é™©
                high_risk_periods.append(col)
        
        if high_risk_periods:
            insights.append(f"ğŸ’¡ {', '.join(high_risk_periods)}æœŸæ•°é€¾æœŸç‡è¶…è¿‡10%ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å’Œé£é™©æ§åˆ¶")
        
        # è¶‹åŠ¿æ´å¯Ÿ
        rising_trends = [col for col in trends if trends[col]["direction"] == "ä¸Šå‡"]
        if rising_trends:
            insights.append(f"ğŸ’¡ {', '.join(rising_trends)}æœŸæ•°é€¾æœŸç‡å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¯èƒ½åæ˜ é£é™©ç§¯ç´¯æˆ–å¤–éƒ¨ç¯å¢ƒå˜åŒ–")
        
        # MOBæœŸæ•°å¯¹æ¯”æ´å¯Ÿ
        if len(mob_columns) > 1:
            mob_rates = [(col, statistics[col]["mean"]) for col in mob_columns if col in statistics]
            mob_rates.sort(key=lambda x: x[1])
            insights.append(f"ğŸ’¡ é€¾æœŸç‡éšMOBæœŸæ•°é€’å¢ï¼Œ{mob_rates[0][0]}æœ€ä½({mob_rates[0][1]:.2%})ï¼Œ{mob_rates[-1][0]}æœ€é«˜({mob_rates[-1][1]:.2%})")
        
        insights.append("ğŸ’¡ é€¾æœŸç‡æ•°æ®ä¸ºé£é™©å®šä»·ã€é¢åº¦ç®¡ç†å’Œå‚¬æ”¶ç­–ç•¥åˆ¶å®šæä¾›äº†é‡åŒ–åŸºç¡€")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        # åŸºäºå…·ä½“æ•°å€¼çš„å»ºè®®
        for col in high_risk_periods:
            recommendations.append(f"ğŸ¯ é’ˆå¯¹{col}æœŸæ•°é«˜é€¾æœŸç‡({statistics[col]['mean']:.2%})ï¼Œå»ºè®®åŠ å¼ºè¯¥æœŸæ•°å®¢æˆ·çš„è·Ÿè¸ªç®¡ç†")
        
        # åŸºäºè¶‹åŠ¿çš„å»ºè®®
        for col in rising_trends:
            recommendations.append(f"ğŸ¯ {col}æœŸæ•°é€¾æœŸç‡ä¸Šå‡è¶‹åŠ¿éœ€è¦æ·±å…¥åˆ†æåŸå› ï¼Œè€ƒè™‘è°ƒæ•´é£æ§ç­–ç•¥")
        
        recommendations.append("ğŸ¯ å»ºç«‹MOBæœŸæ•°é€¾æœŸç‡é¢„è­¦æœºåˆ¶ï¼Œè®¾ç½®é˜ˆå€¼è¿›è¡Œå®æ—¶ç›‘æ§")
        recommendations.append(f"ğŸ¯ åŸºäºå½“å‰æ•°æ®ç‰¹å¾ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨é€¾æœŸç‡è¶…è¿‡{np.mean([statistics[col]['mean'] for col in statistics]):.1%}çš„å®¢æˆ·ç¾¤ä½“")
        
        # æ–¹æ³•è®º
        methodology = f"ğŸ”¬ åŸºäº{len(df)}æ¡çœŸå®ä¸šåŠ¡æ•°æ®çš„ç»Ÿè®¡åˆ†æï¼Œé‡‡ç”¨æè¿°æ€§ç»Ÿè®¡æ–¹æ³•è®¡ç®—å„MOBæœŸæ•°çš„å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰æŒ‡æ ‡ï¼Œç»“åˆè¶‹åŠ¿åˆ†æè¯†åˆ«é£é™©æ¨¡å¼ã€‚æ•°æ®å®Œæ•´åº¦{data_quality['completeness']:.1%}ï¼Œåˆ†æç»“æœå…·æœ‰è¾ƒé«˜å¯ä¿¡åº¦ã€‚"
        
        return {
            "summary": summary,
            "key_findings": key_findings,
            "insights": insights,
            "recommendations": recommendations,
            "methodology": methodology
        }
    
    def _generate_time_series_analysis(self, df: pd.DataFrame, user_input: str, data_insights: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Š"""
        
        time_columns = data_insights["patterns"].get("time_columns", [])
        statistics = data_insights["statistics"]
        trends = data_insights["trends"]
        
        summary = f"åŸºäº{len(df)}æ¡è®°å½•çš„æ—¶é—´åºåˆ—åˆ†æï¼Œæ•°æ®è·¨åº¦æ¶µç›–{len(df)}ä¸ªæ—¶é—´ç‚¹ï¼Œä¸ºè¶‹åŠ¿è¯†åˆ«å’Œé¢„æµ‹æä¾›äº†æ•°æ®åŸºç¡€ã€‚"
        
        key_findings = []
        key_findings.append(f"ğŸ” æ—¶é—´åºåˆ—æ•°æ®åŒ…å«{len(df)}ä¸ªè§‚æµ‹ç‚¹ï¼Œ{len(data_insights['numeric_columns'])}ä¸ªæ•°å€¼æŒ‡æ ‡")
        
        # åˆ†ææ•°å€¼åˆ—çš„æ—¶é—´è¶‹åŠ¿
        for col in data_insights["numeric_columns"][:3]:
            if col in statistics:
                stats = statistics[col]
                key_findings.append(f"ğŸ” {col}åœ¨è§‚æµ‹æœŸå†…å¹³å‡å€¼{stats['mean']:.3f}ï¼Œæ³¢åŠ¨èŒƒå›´{stats['min']:.3f}-{stats['max']:.3f}")
        
        # è¶‹åŠ¿åˆ†æ
        for col in trends:
            trend = trends[col]
            key_findings.append(f"ğŸ” {col}å‘ˆ{trend['direction']}è¶‹åŠ¿ï¼Œä»{trend['first_value']:.3f}å˜åŒ–åˆ°{trend['last_value']:.3f}")
        
        insights = [
            "ğŸ’¡ æ—¶é—´åºåˆ—æ•°æ®åæ˜ äº†ä¸šåŠ¡æŒ‡æ ‡çš„åŠ¨æ€å˜åŒ–è¿‡ç¨‹",
            "ğŸ’¡ è¶‹åŠ¿åˆ†ææœ‰åŠ©äºè¯†åˆ«ä¸šåŠ¡å‘å±•çš„å…³é”®è½¬æŠ˜ç‚¹",
            "ğŸ’¡ æ•°æ®æ³¢åŠ¨å¯èƒ½ä¸å­£èŠ‚æ€§å› ç´ ã€æ”¿ç­–å˜åŒ–æˆ–å¸‚åœºç¯å¢ƒç›¸å…³",
            "ğŸ’¡ æŒç»­ç›‘æ§æ—¶é—´åºåˆ—æŒ‡æ ‡æœ‰åŠ©äºåŠæ—¶å‘ç°å¼‚å¸¸å’Œæœºä¼š"
        ]
        
        recommendations = [
            "ğŸ¯ å»ºç«‹æ—¶é—´åºåˆ—é¢„è­¦æœºåˆ¶ï¼Œç›‘æ§å…³é”®æŒ‡æ ‡çš„å¼‚å¸¸å˜åŒ–",
            "ğŸ¯ ç»“åˆå¤–éƒ¨å› ç´ åˆ†ææ•°æ®æ³¢åŠ¨çš„æ ¹æœ¬åŸå› ",
            "ğŸ¯ å»ºç«‹é¢„æµ‹æ¨¡å‹ï¼ŒåŸºäºå†å²è¶‹åŠ¿é¢„æµ‹æœªæ¥èµ°åŠ¿",
            "ğŸ¯ å®šæœŸè¯„ä¼°æ—¶é—´åºåˆ—æ¨¡å¼ï¼Œä¼˜åŒ–ä¸šåŠ¡ç­–ç•¥"
        ]
        
        methodology = f"ğŸ”¬ é‡‡ç”¨æ—¶é—´åºåˆ—åˆ†ææ–¹æ³•ï¼ŒåŸºäº{len(df)}ä¸ªæ—¶é—´ç‚¹çš„çœŸå®æ•°æ®è®¡ç®—è¶‹åŠ¿ã€æ³¢åŠ¨å’Œå‘¨æœŸæ€§ç‰¹å¾ã€‚é€šè¿‡æè¿°æ€§ç»Ÿè®¡å’Œè¶‹åŠ¿åˆ†æè¯†åˆ«æ•°æ®æ¨¡å¼ã€‚"
        
        return {
            "summary": summary,
            "key_findings": key_findings,
            "insights": insights,
            "recommendations": recommendations,
            "methodology": methodology
        }
    
    def _generate_general_analysis(self, df: pd.DataFrame, user_input: str, data_insights: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆé€šç”¨æ•°æ®åˆ†ææŠ¥å‘Š"""
        
        statistics = data_insights["statistics"]
        
        summary = f"åŸºäº{len(df)}æ¡è®°å½•ã€{len(df.columns)}ä¸ªå­—æ®µçš„æ•°æ®åˆ†æï¼Œä¸ºä¸šåŠ¡ç†è§£å’Œå†³ç­–æ”¯æŒæä¾›äº†é‡åŒ–ä¾æ®ã€‚"
        
        key_findings = []
        key_findings.append(f"ğŸ” æ•°æ®é›†åŒ…å«{len(df)}æ¡è®°å½•ï¼Œ{len(data_insights['numeric_columns'])}ä¸ªæ•°å€¼å­—æ®µï¼Œ{len(data_insights['categorical_columns'])}ä¸ªåˆ†ç±»å­—æ®µ")
        
        # æ•°å€¼å­—æ®µåˆ†æ
        for col in data_insights["numeric_columns"][:3]:
            if col in statistics:
                stats = statistics[col]
                key_findings.append(f"ğŸ” {col}å¹³å‡å€¼{stats['mean']:.3f}ï¼Œæ ‡å‡†å·®{stats['std']:.3f}ï¼Œæ•°æ®åˆ†å¸ƒ{'é›†ä¸­' if stats['std']/stats['mean'] < 0.5 else 'åˆ†æ•£'}")
        
        # æ•°æ®è´¨é‡
        data_quality = data_insights["patterns"]["data_quality"]
        key_findings.append(f"ğŸ” æ•°æ®å®Œæ•´åº¦{data_quality['completeness']:.1%}ï¼Œç¼ºå¤±å€¼æ¯”ä¾‹{data_quality['null_percentage']:.1%}")
        
        insights = [
            "ğŸ’¡ æ•°æ®åˆ†æç»“æœæä¾›äº†ä¸šåŠ¡ç°çŠ¶çš„é‡åŒ–è§†å›¾",
            "ğŸ’¡ ç»Ÿè®¡æŒ‡æ ‡æœ‰åŠ©äºè¯†åˆ«æ•°æ®ä¸­çš„å…³é”®æ¨¡å¼å’Œå¼‚å¸¸",
            "ğŸ’¡ æ•°æ®è´¨é‡è¯„ä¼°ä¸ºåç»­åˆ†æçš„å¯ä¿¡åº¦æä¾›äº†å‚è€ƒ",
            "ğŸ’¡ å¤šç»´åº¦æ•°æ®åˆ†ææ”¯æŒæ›´å…¨é¢çš„ä¸šåŠ¡ç†è§£"
        ]
        
        recommendations = [
            "ğŸ¯ åŸºäºæ•°æ®åˆ†æç»“æœåˆ¶å®šé’ˆå¯¹æ€§çš„ä¸šåŠ¡ç­–ç•¥",
            "ğŸ¯ å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶ï¼Œç¡®ä¿åˆ†æç»“æœçš„å¯é æ€§",
            "ğŸ¯ æ·±å…¥åˆ†æå…³é”®æŒ‡æ ‡çš„é©±åŠ¨å› ç´ å’Œå½±å“æœºåˆ¶",
            "ğŸ¯ å®šæœŸæ›´æ–°æ•°æ®åˆ†æï¼Œè·Ÿè¸ªä¸šåŠ¡å˜åŒ–è¶‹åŠ¿"
        ]
        
        methodology = f"ğŸ”¬ é‡‡ç”¨æè¿°æ€§ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼ŒåŸºäº{len(df)}æ¡çœŸå®æ•°æ®è®¡ç®—å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ç­‰ç»Ÿè®¡æŒ‡æ ‡ï¼Œç»“åˆæ•°æ®è´¨é‡è¯„ä¼°æä¾›ç»¼åˆåˆ†æç»“æœã€‚"
        
        return {
            "summary": summary,
            "key_findings": key_findings,
            "insights": insights,
            "recommendations": recommendations,
            "methodology": methodology
        }
    
    def _generate_empty_data_report(self, user_input: str, sql: str) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºæ•°æ®æŠ¥å‘Š"""
        return {
            "summary": f"é’ˆå¯¹æŸ¥è¯¢'{user_input}'æ‰§è¡ŒSQLåæœªè¿”å›æ•°æ®ï¼Œå¯èƒ½æ˜¯æŸ¥è¯¢æ¡ä»¶è¿‡äºä¸¥æ ¼æˆ–æ•°æ®ç¡®å®ä¸å­˜åœ¨ã€‚",
            "key_findings": [
                "ğŸ” SQLæŸ¥è¯¢æ‰§è¡ŒæˆåŠŸä½†æœªè¿”å›ä»»ä½•è®°å½•",
                "ğŸ” å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼šæŸ¥è¯¢æ¡ä»¶è¿‡äºä¸¥æ ¼ã€æ•°æ®æ—¶é—´èŒƒå›´ä¸åŒ¹é…ã€è¡¨ä¸­ç¡®å®æ— ç›¸å…³æ•°æ®",
                "ğŸ” å»ºè®®æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶çš„åˆç†æ€§å’Œæ•°æ®æºçš„å®Œæ•´æ€§",
                "ğŸ” å¯ä»¥å°è¯•æ”¾å®½æŸ¥è¯¢æ¡ä»¶æˆ–æ‰©å¤§æ—¶é—´èŒƒå›´",
                "ğŸ” ç¡®è®¤ç›¸å…³ä¸šåŠ¡æ•°æ®æ˜¯å¦å·²æ­£ç¡®å½•å…¥ç³»ç»Ÿ"
            ],
            "insights": [
                "ğŸ’¡ ç©ºç»“æœå¯èƒ½åæ˜ äº†ä¸šåŠ¡çš„çœŸå®çŠ¶å†µï¼Œä¹Ÿå¯èƒ½æ˜¯æŸ¥è¯¢é€»è¾‘çš„é—®é¢˜",
                "ğŸ’¡ å»ºè®®ç»“åˆä¸šåŠ¡èƒŒæ™¯åˆ¤æ–­ç©ºç»“æœçš„åˆç†æ€§",
                "ğŸ’¡ å¯ä»¥é€šè¿‡è°ƒæ•´æŸ¥è¯¢å‚æ•°æ¥éªŒè¯æ•°æ®å­˜åœ¨æ€§",
                "ğŸ’¡ ç©ºç»“æœä¹Ÿæ˜¯ä¸€ç§æœ‰ä»·å€¼çš„ä¸šåŠ¡ä¿¡æ¯"
            ],
            "recommendations": [
                "ğŸ¯ æ£€æŸ¥å¹¶è°ƒæ•´æŸ¥è¯¢æ¡ä»¶ï¼Œç¡®ä¿å‚æ•°è®¾ç½®åˆç†",
                "ğŸ¯ éªŒè¯æ•°æ®æºçš„å®Œæ•´æ€§å’Œæ—¶æ•ˆæ€§",
                "ğŸ¯ å°è¯•æ›´å®½æ³›çš„æŸ¥è¯¢æ¡ä»¶ä»¥ç¡®è®¤æ•°æ®å­˜åœ¨æ€§",
                "ğŸ¯ å¦‚ç¡®è®¤æ— æ•°æ®ï¼Œè€ƒè™‘è¿™ä¸€ç»“æœçš„ä¸šåŠ¡å«ä¹‰"
            ],
            "methodology": "ğŸ”¬ åŸºäºSQLæŸ¥è¯¢ç»“æœçš„ç©ºå€¼åˆ†æï¼Œç»“åˆæŸ¥è¯¢é€»è¾‘å’Œä¸šåŠ¡èƒŒæ™¯è¿›è¡ŒåŸå› æ¨æµ‹å’Œå»ºè®®ç”Ÿæˆã€‚"
        }
    
    def _generate_fallback_report(self, user_input: str, sql: str, error_msg: str) -> Dict[str, Any]:
        """ç”Ÿæˆå…œåº•æŠ¥å‘Š"""
        return {
            "summary": f"åœ¨ç”Ÿæˆæ•°æ®é©±åŠ¨åˆ†ææŠ¥å‘Šæ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œå·²åˆ‡æ¢åˆ°åŸºç¡€åˆ†ææ¨¡å¼ã€‚",
            "key_findings": [
                "ğŸ” æ•°æ®åˆ†æè¿‡ç¨‹ä¸­é‡åˆ°æŠ€æœ¯å¼‚å¸¸",
                "ğŸ” å·²æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è·å–ç»“æœæ•°æ®",
                "ğŸ” å»ºè®®æ£€æŸ¥æ•°æ®æ ¼å¼å’Œç»“æ„çš„å®Œæ•´æ€§",
                "ğŸ” ç³»ç»Ÿå·²è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ä¾›æŠ€æœ¯å›¢é˜Ÿåˆ†æ",
                "ğŸ” åŸºç¡€çš„æŸ¥è¯¢ç»“æœå±•ç¤ºåŠŸèƒ½æ­£å¸¸å·¥ä½œ"
            ],
            "insights": [
                "ğŸ’¡ æŠ€æœ¯å¼‚å¸¸ä¸å½±å“SQLæŸ¥è¯¢çš„æ­£ç¡®æ‰§è¡Œ",
                "ğŸ’¡ æ•°æ®ç»“æœä»ç„¶å¯ç”¨äºäººå·¥åˆ†æ",
                "ğŸ’¡ å»ºè®®å…³æ³¨æ•°æ®è´¨é‡å’Œæ ¼å¼è§„èŒƒæ€§",
                "ğŸ’¡ ç³»ç»Ÿæ­£åœ¨æŒç»­ä¼˜åŒ–åˆ†æç®—æ³•çš„ç¨³å®šæ€§"
            ],
            "recommendations": [
                "ğŸ¯ å¯ä»¥åŸºäºæŸ¥è¯¢ç»“æœè¿›è¡Œäººå·¥åˆ†æ",
                "ğŸ¯ å¦‚éœ€è¯¦ç»†åˆ†ææŠ¥å‘Šï¼Œå»ºè®®è”ç³»æŠ€æœ¯æ”¯æŒ",
                "ğŸ¯ æ£€æŸ¥åŸå§‹æ•°æ®çš„æ ¼å¼å’Œå®Œæ•´æ€§",
                "ğŸ¯ è€ƒè™‘ç®€åŒ–æŸ¥è¯¢é€»è¾‘ä»¥æé«˜åˆ†ææˆåŠŸç‡"
            ],
            "methodology": f"ğŸ”¬ åŸºç¡€åˆ†ææ¨¡å¼ï¼ŒæŠ€æœ¯å¼‚å¸¸ä¿¡æ¯ï¼š{error_msg[:100]}..."
        } 