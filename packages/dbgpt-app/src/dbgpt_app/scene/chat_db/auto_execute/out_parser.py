import json
import logging
import xml.etree.ElementTree as ET
from typing import Dict, NamedTuple, Optional
import re
from datetime import datetime

import numpy as np
import pandas as pd
import sqlparse
from sqlalchemy.exc import SQLAlchemyError
import pymysql

from dbgpt._private.config import Config
from dbgpt.core.interface.output_parser import BaseOutputParser
from dbgpt.util.json_utils import serialize

from ...exceptions import AppActionException
from .sql_fixer import create_sql_fixer

CFG = Config()


class TimeAndReportFixer:
    """æ—¶é—´è§£æå’ŒæŠ¥å‘Šç”Ÿæˆä¿®å¤å™¨"""
    
    def __init__(self):
        self.current_year = datetime.now().year
        self.current_month = datetime.now().month
        self.current_date = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ†ææŠ¥å‘Šå…³é”®è¯
        self.analysis_keywords = [
            'åˆ†æ', 'æŠ¥å‘Š', 'æ€»ç»“', 'æ ¹å› ', 'åŸå› åˆ†æ',
            'analysis', 'analyze', 'report', 'summary', 'root cause'
        ]
    
    def check_analysis_request(self, user_input: str) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¯·æ±‚åˆ†ææŠ¥å‘Š"""
        if not user_input:
            return False
            
        user_input_lower = user_input.lower()
        
        for keyword in self.analysis_keywords:
            if keyword.lower() in user_input_lower:
                logger.info(f"æ£€æµ‹åˆ°åˆ†æå…³é”®è¯: '{keyword}'")
                return True
        
        return False
    
    def fix_sql_time_references(self, sql: str) -> str:
        """ä¿®å¤SQLä¸­çš„æ—¶é—´å¼•ç”¨"""
        if not sql:
            return sql
        
        # æ›¿æ¢ç¡¬ç¼–ç çš„2023å¹´ä»½
        fixed_sql = re.sub(r"'2023-(\d{2})'", f"'{self.current_year}-\\1'", sql)
        
        # ä¿®å¤å¯èƒ½å¯¼è‡´é‡å¤åˆ—åçš„SQLæ¨¡å¼
        fixed_sql = self._fix_duplicate_column_sql(fixed_sql)
        
        if fixed_sql != sql:
            logger.info(f"SQLæ—¶é—´ä¿®å¤: {sql} -> {fixed_sql}")
        
        return fixed_sql
    
    def _fix_duplicate_column_sql(self, sql: str) -> str:
        """ä¿®å¤å¯èƒ½å¯¼è‡´é‡å¤åˆ—åçš„SQL"""
        if not sql:
            return sql
        
        # æ£€æµ‹ SELECT ld.*, li.* è¿™æ ·çš„æ¨¡å¼
        pattern = r'SELECT\s+(\w+)\.\*\s*,\s*(\w+)\.\*'
        match = re.search(pattern, sql, re.IGNORECASE)
        
        if match:
            table1_alias = match.group(1)
            table2_alias = match.group(2)
            
            logger.info(f"æ£€æµ‹åˆ°å¯èƒ½å¯¼è‡´é‡å¤åˆ—åçš„SQLæ¨¡å¼: {table1_alias}.*, {table2_alias}.*")
            
            # æ›¿æ¢ä¸ºæ˜ç¡®çš„åˆ—é€‰æ‹©ï¼ˆè¿™é‡Œæä¾›ä¸€ä¸ªåŸºæœ¬çš„ä¿®å¤ï¼‰
            # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥è·å–å®é™…çš„è¡¨ç»“æ„
            replacement = f"""SELECT 
    {table1_alias}.loan_id AS '{table1_alias}_loan_id',
    {table1_alias}.overdue_amount AS '{table1_alias}_overdue_amount',
    {table1_alias}.repayment_status AS 'è¿˜æ¬¾çŠ¶æ€',
    {table1_alias}.repayment_date AS 'è¿˜æ¬¾æ—¥æœŸ',
    {table2_alias}.loan_amount AS 'è´·æ¬¾é‡‘é¢',
    {table2_alias}.interest_rate AS 'åˆ©ç‡',
    {table2_alias}.customer_id AS 'å®¢æˆ·ID'"""
            
            fixed_sql = re.sub(pattern, replacement, sql, flags=re.IGNORECASE)
            logger.info(f"SQLé‡å¤åˆ—ä¿®å¤: å·²å°† {table1_alias}.*, {table2_alias}.* æ›¿æ¢ä¸ºæ˜ç¡®çš„åˆ—é€‰æ‹©")
            
            return fixed_sql
        
        return sql
    
    def ensure_analysis_report_in_response(self, response_dict: Dict, user_input: str = "") -> Dict:
        """ç¡®ä¿å“åº”ä¸­åŒ…å«åˆ†ææŠ¥å‘Šï¼ˆå¦‚æœç”¨æˆ·è¯·æ±‚äº†ï¼‰"""
        if not self.check_analysis_request(user_input):
            return response_dict
        
        # å¦‚æœç”¨æˆ·è¯·æ±‚åˆ†æä½†å“åº”ä¸­æ²¡æœ‰analysis_report
        if 'analysis_report' not in response_dict or not response_dict['analysis_report']:
            logger.info("ç”¨æˆ·è¯·æ±‚åˆ†æä½†å“åº”ä¸­ç¼ºå°‘analysis_reportï¼Œæ­£åœ¨æ·»åŠ æ™ºèƒ½åˆ†ææŠ¥å‘Š...")
            
            # åŸºäºç”¨æˆ·è¾“å…¥å’ŒSQLç”Ÿæˆæ›´æ™ºèƒ½çš„åˆ†ææŠ¥å‘Š
            sql = response_dict.get('sql', '')
            analysis_report = self._generate_intelligent_analysis_report(user_input, sql)
            
            response_dict['analysis_report'] = analysis_report
            logger.info("å·²æ·»åŠ æ™ºèƒ½åˆ†ææŠ¥å‘Šç»“æ„")
        
        return response_dict
    
    def _generate_intelligent_analysis_report(self, user_input: str, sql: str) -> Dict:
        """åŸºäºç”¨æˆ·è¾“å…¥å’ŒSQLç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š"""
        
        # åˆ†æç”¨æˆ·æ„å›¾å’ŒSQLå†…å®¹
        is_dpd_analysis = any(keyword in user_input.lower() for keyword in ['dpd', 'é€¾æœŸ', 'overdue'])
        is_time_series = any(keyword in sql.lower() for keyword in ['group by', 'order by', 'date', 'month', 'year'])
        is_rate_analysis = any(keyword in sql.lower() for keyword in ['rate', 'ç‡', 'percentage', '%'])
        
        # ç”Ÿæˆé’ˆå¯¹æ€§çš„åˆ†ææŠ¥å‘Š
        if is_dpd_analysis and is_time_series:
            return {
                "summary": f"åŸºäºç”¨æˆ·æŸ¥è¯¢'{user_input}'çš„DPDé€¾æœŸç‡æ—¶é—´åºåˆ—åˆ†æï¼Œé€šè¿‡SQLæŸ¥è¯¢è·å–å„æ—¶é—´æ®µçš„é€¾æœŸè¡¨ç°æ•°æ®ï¼Œä¸ºé£é™©ç®¡ç†æä¾›æ•°æ®æ”¯æŒã€‚",
                "key_findings": [
                    "ğŸ” DPDé€¾æœŸç‡æ•°æ®æŒ‰æ—¶é—´ç»´åº¦è¿›è¡Œäº†åˆ†ç»„ç»Ÿè®¡ï¼Œä¾¿äºè¯†åˆ«è¶‹åŠ¿å˜åŒ–",
                    "ğŸ” æŸ¥è¯¢ç»“æœæ¶µç›–äº†å¤šä¸ªæ—¶é—´å‘¨æœŸçš„é€¾æœŸè¡¨ç°ï¼Œå¯è¿›è¡ŒåŒæ¯”ç¯æ¯”åˆ†æ", 
                    "ğŸ” æ•°æ®ç»“æ„æ”¯æŒæŒ‰æœˆåº¦/å­£åº¦ç»´åº¦è¿›è¡Œé€¾æœŸç‡æ³¢åŠ¨åˆ†æ",
                    "ğŸ” é€¾æœŸç‡æŒ‡æ ‡å¯ç”¨äºè¯„ä¼°èµ„äº§è´¨é‡å’Œé£é™©æ°´å¹³å˜åŒ–",
                    "ğŸ” æ—¶é—´åºåˆ—æ•°æ®æœ‰åŠ©äºè¯†åˆ«å­£èŠ‚æ€§å› ç´ å¯¹é€¾æœŸç‡çš„å½±å“"
                ],
                "insights": [
                    "ğŸ’¡ DPDé€¾æœŸç‡çš„æ—¶é—´è¶‹åŠ¿åæ˜ äº†ä¸šåŠ¡é£é™©ç®¡ç†æ•ˆæœå’Œå¤–éƒ¨ç¯å¢ƒå½±å“",
                    "ğŸ’¡ é€¾æœŸç‡æ³¢åŠ¨å¯èƒ½ä¸å®è§‚ç»æµç¯å¢ƒã€æ”¿ç­–å˜åŒ–ã€ä¸šåŠ¡ç­–ç•¥è°ƒæ•´ç›¸å…³",
                    "ğŸ’¡ æŒç»­ç›‘æ§DPDæŒ‡æ ‡æœ‰åŠ©äºåŠæ—¶å‘ç°é£é™©ä¿¡å·å¹¶é‡‡å–é¢„é˜²æªæ–½",
                    "ğŸ’¡ ä¸åŒæ—¶é—´æ®µçš„é€¾æœŸç‡å¯¹æ¯”å¯ä»¥è¯„ä¼°é£æ§ç­–ç•¥çš„æœ‰æ•ˆæ€§"
                ],
                "recommendations": [
                    "ğŸ¯ å»ºç«‹DPDé€¾æœŸç‡é¢„è­¦æœºåˆ¶ï¼Œè®¾ç½®åˆç†çš„é˜ˆå€¼è¿›è¡Œå®æ—¶ç›‘æ§",
                    "ğŸ¯ å®šæœŸåˆ†æé€¾æœŸç‡å˜åŒ–çš„æ ¹æœ¬åŸå› ï¼ŒåŒ…æ‹¬å®¢æˆ·ç»“æ„ã€äº§å“ç‰¹æ€§ã€å¤–éƒ¨å› ç´ ",
                    "ğŸ¯ ç»“åˆä¸šåŠ¡æ•°æ®è¿›è¡Œå¤šç»´åº¦åˆ†æï¼Œå¦‚æŒ‰å®¢æˆ·ç¾¤ä½“ã€äº§å“ç±»å‹ã€åœ°åŒºç­‰ç»†åˆ†",
                    "ğŸ¯ å»ºç«‹é€¾æœŸç‡é¢„æµ‹æ¨¡å‹ï¼Œæå‰è¯†åˆ«æ½œåœ¨é£é™©å¹¶åˆ¶å®šåº”å¯¹ç­–ç•¥"
                ],
                "methodology": "ğŸ”¬ é‡‡ç”¨SQLæ—¶é—´åºåˆ—åˆ†ææ–¹æ³•ï¼Œé€šè¿‡GROUP BYæ—¶é—´ç»´åº¦ç»Ÿè®¡DPDé€¾æœŸç‡ï¼Œç»“åˆORDER BYè¿›è¡Œæ—¶é—´æ’åºï¼Œç¡®ä¿æ•°æ®çš„æ—¶åºæ€§å’Œå¯æ¯”æ€§ã€‚åˆ†æé€»è¾‘åŸºäºå†å²æ•°æ®è¶‹åŠ¿è¯†åˆ«å’Œä¸šåŠ¡é£é™©è¯„ä¼°æ¡†æ¶ã€‚"
            }
        elif is_rate_analysis:
            return {
                "summary": f"é’ˆå¯¹ç”¨æˆ·æŸ¥è¯¢'{user_input}'çš„æ¯”ç‡åˆ†æï¼Œé€šè¿‡æ•°æ®ç»Ÿè®¡è®¡ç®—ç›¸å…³æŒ‡æ ‡çš„æ¯”ç‡è¡¨ç°ï¼Œä¸ºä¸šåŠ¡å†³ç­–æä¾›é‡åŒ–ä¾æ®ã€‚",
                "key_findings": [
                    "ğŸ” æ¯”ç‡æŒ‡æ ‡èƒ½å¤Ÿæ ‡å‡†åŒ–ä¸åŒè§„æ¨¡æ•°æ®çš„æ¯”è¾ƒåˆ†æ",
                    "ğŸ” æŸ¥è¯¢ç»“æœæä¾›äº†å…³é”®ä¸šåŠ¡æŒ‡æ ‡çš„æ¯”ç‡è¡¨ç°æ•°æ®",
                    "ğŸ” æ¯”ç‡åˆ†ææœ‰åŠ©äºè¯†åˆ«ä¸šåŠ¡è¡¨ç°çš„ç›¸å¯¹æ°´å¹³å’Œå˜åŒ–è¶‹åŠ¿",
                    "ğŸ” æ•°æ®ç»“æ„æ”¯æŒè¿›è¡ŒåŸºå‡†å¯¹æ¯”å’Œè¡Œä¸šæ ‡å‡†è¯„ä¼°",
                    "ğŸ” æ¯”ç‡æŒ‡æ ‡å¯ç”¨äºè¯„ä¼°ä¸šåŠ¡æ•ˆç‡å’Œé£é™©æ°´å¹³"
                ],
                "insights": [
                    "ğŸ’¡ æ¯”ç‡åˆ†ææä¾›äº†æ ‡å‡†åŒ–çš„ä¸šåŠ¡è¡¨ç°è¯„ä¼°è§†è§’",
                    "ğŸ’¡ æ¯”ç‡å˜åŒ–è¶‹åŠ¿åæ˜ äº†ä¸šåŠ¡è¿è¥æ•ˆç‡å’Œå¸‚åœºç¯å¢ƒå½±å“",
                    "ğŸ’¡ é€šè¿‡æ¯”ç‡å¯¹æ¯”å¯ä»¥è¯†åˆ«ä¸šåŠ¡ä¼˜åŠ¿å’Œæ”¹è¿›ç©ºé—´",
                    "ğŸ’¡ æ¯”ç‡æŒ‡æ ‡æ˜¯åˆ¶å®šä¸šåŠ¡ç›®æ ‡å’Œè¯„ä¼°ç­–ç•¥æ•ˆæœçš„é‡è¦ä¾æ®"
                ],
                "recommendations": [
                    "ğŸ¯ å»ºç«‹æ¯”ç‡æŒ‡æ ‡çš„åŸºå‡†å€¼å’Œç›®æ ‡å€¼ï¼Œç”¨äºä¸šåŠ¡è¡¨ç°è¯„ä¼°",
                    "ğŸ¯ å®šæœŸç›‘æ§å…³é”®æ¯”ç‡æŒ‡æ ‡çš„å˜åŒ–ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸æƒ…å†µ",
                    "ğŸ¯ ç»“åˆè¡Œä¸šæ ‡å‡†è¿›è¡Œæ¯”ç‡å¯¹æ¯”åˆ†æï¼Œè¯„ä¼°ç«äº‰åœ°ä½",
                    "ğŸ¯ å»ºç«‹æ¯”ç‡æŒ‡æ ‡çš„é¢„è­¦æœºåˆ¶ï¼Œç¡®ä¿ä¸šåŠ¡é£é™©å¯æ§"
                ],
                "methodology": "ğŸ”¬ é‡‡ç”¨æ¯”ç‡åˆ†ææ–¹æ³•ï¼Œé€šè¿‡SQLè®¡ç®—ç›¸å…³æŒ‡æ ‡çš„æ¯”ç‡å€¼ï¼Œç»“åˆç»Ÿè®¡åˆ†æè¯†åˆ«æ•°æ®ç‰¹å¾å’Œå˜åŒ–è¶‹åŠ¿ã€‚åˆ†ææ¡†æ¶åŸºäºè´¢åŠ¡åˆ†æå’Œä¸šåŠ¡ç»©æ•ˆè¯„ä¼°çš„æ ‡å‡†æ–¹æ³•è®ºã€‚"
            }
        else:
            # é€šç”¨åˆ†ææŠ¥å‘Š
            return {
                "summary": f"åŸºäºç”¨æˆ·æŸ¥è¯¢'{user_input}'çš„æ•°æ®åˆ†æï¼Œé€šè¿‡SQLæŸ¥è¯¢æå–ç›¸å…³ä¸šåŠ¡æ•°æ®ï¼Œä¸ºå†³ç­–æ”¯æŒæä¾›æ•°æ®æ´å¯Ÿã€‚",
                "key_findings": [
                    "ğŸ” æ•°æ®æŸ¥è¯¢æˆåŠŸæ‰§è¡Œï¼Œè·å–äº†ç”¨æˆ·å…³æ³¨çš„ä¸šåŠ¡æŒ‡æ ‡æ•°æ®",
                    "ğŸ” æŸ¥è¯¢ç»“æœç»“æ„åŒ–å±•ç¤ºï¼Œä¾¿äºè¿›è¡Œæ•°æ®åˆ†æå’Œè§£è¯»",
                    "ğŸ” æ•°æ®è¦†ç›–äº†ç”¨æˆ·å…³å¿ƒçš„ä¸šåŠ¡ç»´åº¦å’Œæ—¶é—´èŒƒå›´",
                    "ğŸ” æŸ¥è¯¢é€»è¾‘ç¬¦åˆä¸šåŠ¡åˆ†æéœ€æ±‚ï¼Œæ•°æ®å‡†ç¡®æ€§å¾—åˆ°ä¿éšœ",
                    "ğŸ” ç»“æœæ•°æ®å¯ç”¨äºè¿›ä¸€æ­¥çš„ç»Ÿè®¡åˆ†æå’Œè¶‹åŠ¿è¯†åˆ«"
                ],
                "insights": [
                    "ğŸ’¡ æ•°æ®åˆ†æç»“æœåæ˜ äº†å½“å‰ä¸šåŠ¡çŠ¶æ€å’Œå…³é”®æŒ‡æ ‡è¡¨ç°",
                    "ğŸ’¡ é€šè¿‡æ•°æ®å¯ä»¥è¯†åˆ«ä¸šåŠ¡è¿è¥ä¸­çš„æ¨¡å¼å’Œè¶‹åŠ¿",
                    "ğŸ’¡ æ•°æ®æ´å¯Ÿæœ‰åŠ©äºç†è§£ä¸šåŠ¡é©±åŠ¨å› ç´ å’Œå½±å“æœºåˆ¶",
                    "ğŸ’¡ åˆ†æç»“æœä¸ºä¸šåŠ¡ä¼˜åŒ–å’Œå†³ç­–åˆ¶å®šæä¾›äº†æ•°æ®æ”¯æ’‘"
                ],
                "recommendations": [
                    "ğŸ¯ å»ºç«‹å®šæœŸæ•°æ®ç›‘æ§æœºåˆ¶ï¼ŒæŒç»­è·Ÿè¸ªå…³é”®ä¸šåŠ¡æŒ‡æ ‡",
                    "ğŸ¯ æ·±å…¥åˆ†ææ•°æ®èƒŒåçš„ä¸šåŠ¡é€»è¾‘ï¼Œè¯†åˆ«æ”¹è¿›æœºä¼š",
                    "ğŸ¯ ç»“åˆä¸šåŠ¡ç›®æ ‡åˆ¶å®šæ•°æ®é©±åŠ¨çš„è¡ŒåŠ¨è®¡åˆ’",
                    "ğŸ¯ å»ºç«‹æ•°æ®è´¨é‡ç®¡ç†ä½“ç³»ï¼Œç¡®ä¿åˆ†æç»“æœçš„å¯é æ€§"
                ],
                "methodology": "ğŸ”¬ é‡‡ç”¨ç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€(SQL)è¿›è¡Œæ•°æ®æå–å’Œåˆæ­¥ç»Ÿè®¡ï¼Œç»“åˆä¸šåŠ¡ç†è§£è¿›è¡Œæ•°æ®è§£è¯»å’Œæ´å¯Ÿæå–ã€‚åˆ†ææ–¹æ³•åŸºäºæè¿°æ€§ç»Ÿè®¡å’Œä¸šåŠ¡é€»è¾‘æ¨ç†ã€‚"
            }


class SqlAction(NamedTuple):
    sql: str
    thoughts: Dict
    display: str
    direct_response: str
    missing_info: str = ""
    analysis_report: Dict = {}

    def to_dict(self) -> Dict[str, Dict]:
        return {
            "sql": self.sql,
            "thoughts": self.thoughts,
            "display": self.display,
            "direct_response": self.direct_response,
            "missing_info": self.missing_info,
            "analysis_report": self.analysis_report,
        }


logger = logging.getLogger(__name__)


class DbChatOutputParser(BaseOutputParser):
    def __init__(self, is_stream_out: bool = False, connector=None, **kwargs):
        super().__init__(is_stream_out=is_stream_out, **kwargs)
        self._sql_validator = None
        self._connector = connector
        
        # Auto-initialize SQL validator if connector is provided
        if connector:
            self._initialize_sql_validator()
        self.sql_fixer = create_sql_fixer()
        
        # Initialize time and report fixer
        self.time_report_fixer = TimeAndReportFixer()
        self._current_user_input = ""  # Store current user input for analysis

    def _initialize_sql_validator(self):
        """Initialize SQL validator with the provided connector."""
        try:
            from .sql_validator import SQLValidator
            self._sql_validator = SQLValidator(self._connector)
            logger.info("SQL validator initialized successfully")
        except ImportError as e:
            logger.warning(f"Could not import SQL validator: {e}")
        except Exception as e:
            logger.error(f"Failed to initialize SQL validator: {e}")

    def set_sql_validator(self, validator):
        """Set the SQL validator instance."""
        self._sql_validator = validator

    def set_connector(self, connector):
        """Set the database connector and initialize SQL validator."""
        self._connector = connector
        if connector and not self._sql_validator:
            self._initialize_sql_validator()

    def is_sql_statement(self, statement):
        parsed = sqlparse.parse(statement)
        if not parsed:
            return False
        for stmt in parsed:
            if stmt.get_type() != "UNKNOWN":
                return True
        return False

    def parse_prompt_response(self, model_out_text, user_input: str = ""):
        clean_str = super().parse_prompt_response(model_out_text)
        logger.info(f"=== DEBUG: parse_prompt_response ===")
        logger.info(f"Original model_out_text: {model_out_text}")
        logger.info(f"Clean prompt response: {clean_str}")
        logger.info(f"User input: {user_input}")
        logger.info(f"=== END DEBUG ===")
        
        # Store user input for analysis
        self._current_user_input = user_input
        
        # Compatible with community pure sql output model
        if self.is_sql_statement(clean_str):
            logger.info("Detected pure SQL statement")
            # Apply time fixes to pure SQL
            fixed_sql = self.time_report_fixer.fix_sql_time_references(clean_str)
            return SqlAction(fixed_sql, "", "", "", "", {})
        else:
            try:
                response = json.loads(clean_str, strict=False)
                logger.info(f"Successfully parsed JSON response: {response}")
                
                # Apply time and report fixes to the response
                response = self.time_report_fixer.ensure_analysis_report_in_response(response, user_input)
                
                sql = ""
                thoughts = dict
                display = ""
                resp = ""
                missing_info = ""
                analysis_report = {}
                for key in sorted(response):
                    if key.strip() == "sql":
                        sql = response[key]
                        # Apply time fixes to SQL
                        sql = self.time_report_fixer.fix_sql_time_references(sql)
                    if key.strip() == "thoughts":
                        thoughts = response[key]
                    if key.strip() == "display_type":
                        display = response[key]
                    if key.strip() == "direct_response":
                        resp = response[key]
                    if key.strip() == "missing_info":
                        missing_info = response[key]
                    if key.strip() == "analysis_report":
                        analysis_report = response[key] if isinstance(response[key], dict) else {}
                return SqlAction(
                    sql=sql, thoughts=thoughts, display=display, direct_response=resp, 
                    missing_info=missing_info, analysis_report=analysis_report
                )
            except Exception as e:
                logger.error(f"json load failed: {clean_str}, error: {e}")
                return SqlAction("", clean_str, "", "", "", {})

    def _safe_parse_vector_data_with_pca(self, df):
        """Enhanced PCA parsing with better error handling."""
        try:
            from sklearn.decomposition import PCA
        except ImportError:
            logger.error("scikit-learn not available for PCA processing")
            raise ImportError(
                "Could not import scikit-learn package. "
                "Please install it with `pip install scikit-learn`."
            )

        try:
            nrow, ncol = df.shape
            if nrow == 0 or ncol == 0:
                logger.warning("Empty DataFrame provided for PCA processing")
                return df, False

            vec_col = -1
            for i_col in range(ncol):
                try:
                    first_val = df.iloc[0, i_col]
                    if isinstance(first_val, list):
                        vec_col = i_col
                        break
                    elif isinstance(first_val, bytes):
                        try:
                            decoded_val = json.loads(first_val.decode())
                            if isinstance(decoded_val, list):
                                vec_col = i_col
                                break
                        except (json.JSONDecodeError, UnicodeDecodeError):
                            continue
                    elif isinstance(first_val, str):
                        try:
                            parsed_val = json.loads(first_val)
                            if isinstance(parsed_val, list):
                                vec_col = i_col
                                break
                        except json.JSONDecodeError:
                            continue
                except (IndexError, TypeError):
                    continue
                    
            if vec_col == -1:
                logger.info("No vector column found for PCA processing")
                return df, False
                
            # Validate vector data
            try:
                first_vec = df.iloc[0, vec_col]
                if isinstance(first_vec, bytes):
                    first_vec = json.loads(first_vec.decode())
                elif isinstance(first_vec, str):
                    first_vec = json.loads(first_vec)
                    
                vec_dim = len(first_vec)
                if min(nrow, vec_dim) < 2:
                    logger.warning(f"Insufficient data for PCA: rows={nrow}, dimensions={vec_dim}")
                    return df, False
                    
            except Exception as e:
                logger.error(f"Error validating vector data: {e}")
                return df, False

            # Process vector data
            try:
                if isinstance(df.iloc[0, vec_col], bytes):
                    df.iloc[:, vec_col] = df.iloc[:, vec_col].apply(
                        lambda x: json.loads(x.decode()) if isinstance(x, bytes) else x
                    )
                elif isinstance(df.iloc[0, vec_col], str):
                    df.iloc[:, vec_col] = df.iloc[:, vec_col].apply(
                        lambda x: json.loads(x) if isinstance(x, str) else x
                    )
                    
                X = np.array(df.iloc[:, vec_col].tolist())
                
                # Validate array shape
                if X.ndim != 2:
                    logger.error(f"Invalid vector array shape: {X.shape}")
                    return df, False
                    
                pca = PCA(n_components=2)
                X_pca = pca.fit_transform(X)

                new_df = pd.DataFrame()
                for i_col in range(ncol):
                    if i_col == vec_col:
                        continue
                    col_name = df.columns[i_col]
                    new_df[col_name] = df[col_name]
                new_df["__x"] = [pos[0] for pos in X_pca]
                new_df["__y"] = [pos[1] for pos in X_pca]
                
                logger.info(f"PCA processing successful: {nrow} rows, {vec_dim} dimensions -> 2D")
                return new_df, True
                
            except Exception as e:
                logger.error(f"Error during PCA transformation: {e}")
                return df, False
                
        except Exception as e:
            logger.error(f"Unexpected error in PCA processing: {e}")
            return df, False

    def parse_vector_data_with_pca(self, df):
        """Wrapper for backward compatibility."""
        return self._safe_parse_vector_data_with_pca(df)

    def _create_detailed_error_message(self, error: Exception, sql: str = "", context: str = "") -> str:
        """Create a detailed, user-friendly error message."""
        error_type = type(error).__name__
        error_msg = str(error)
        
        # Categorize common errors
        if "Unknown column" in error_msg or "doesn't exist" in error_msg:
            error_category = "Column Reference Error"
            icon = "ğŸ”"
        elif "syntax error" in error_msg.lower() or "SQL syntax" in error_msg:
            error_category = "SQL Syntax Error"
            icon = "âš ï¸"
        elif "connection" in error_msg.lower() or "connect" in error_msg.lower():
            error_category = "Database Connection Error"
            icon = "ğŸ”Œ"
        elif "permission" in error_msg.lower() or "access" in error_msg.lower():
            error_category = "Permission Error"
            icon = "ğŸ”’"
        elif "timeout" in error_msg.lower():
            error_category = "Timeout Error"
            icon = "â±ï¸"
        else:
            error_category = "General Error"
            icon = "âŒ"
            
        detailed_msg = f"""
<div style="background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 5px; padding: 15px; margin: 10px 0;">
    <h4 style="color: #856404; margin: 0 0 10px 0;">{icon} {error_category}</h4>
    <p style="margin: 5px 0;"><strong>Error:</strong> {error_msg}</p>
    {f'<p style="margin: 5px 0;"><strong>SQL:</strong> <code>{sql}</code></p>' if sql else ''}
    {f'<p style="margin: 5px 0;"><strong>Context:</strong> {context}</p>' if context else ''}
    
    <div style="margin-top: 10px; padding: 10px; background-color: #e8f4f8; border-radius: 3px;">
        <strong>ğŸ’¡ Quick Fixes:</strong>
        <ul style="margin: 5px 0; padding-left: 20px;">
            <li>Check your database schema and column names</li>
            <li>Verify your database connection is working</li>
            <li>Run the diagnostic tool: <code>python debug_view_content_error.py</code></li>
            <li>Check the logs for more detailed information</li>
        </ul>
    </div>
</div>
        """.strip()
        
        return detailed_msg

    def format_sql_error_for_user(self, error: Exception, sql: str = None) -> str:
        """
        Format SQL error for user-friendly display
        å°†SQLé”™è¯¯æ ¼å¼åŒ–ä¸ºç”¨æˆ·å‹å¥½çš„ä¿¡æ¯
        """
        error_msg = str(error)
        
        # Common SQL error patterns and their user-friendly explanations
        error_patterns = {
            r"Unknown column '([^']+)' in 'field list'": "å­—æ®µ '{0}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥å­—æ®µåæ˜¯å¦æ­£ç¡®",
            r"Table '([^']+)' doesn't exist": "è¡¨ '{0}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è¡¨åæ˜¯å¦æ­£ç¡®", 
            r"You have an error in your SQL syntax": "SQLè¯­æ³•é”™è¯¯ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥",
            r"Duplicate column name '([^']+)'": "é‡å¤çš„å­—æ®µå '{0}'",
            r"Unknown table '([^']+)'": "æœªçŸ¥çš„è¡¨ '{0}'",
            r"Column '([^']+)' in field list is ambiguous": "å­—æ®µ '{0}' å­˜åœ¨æ­§ä¹‰ï¼Œéœ€è¦æŒ‡å®šè¡¨å",
        }
        
        # Try to match common patterns
        for pattern, template in error_patterns.items():
            match = re.search(pattern, error_msg)
            if match:
                try:
                    return template.format(*match.groups())
                except:
                    return template
        
        # If no pattern matches, return a generic but informative message
        if "1054" in error_msg:
            return f"å­—æ®µå¼•ç”¨é”™è¯¯: {error_msg}"
        elif "1146" in error_msg:
            return f"è¡¨ä¸å­˜åœ¨é”™è¯¯: {error_msg}"
        elif "1064" in error_msg:
            return f"SQLè¯­æ³•é”™è¯¯: {error_msg}"
        else:
            return f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {error_msg}"

    def validate_sql_basic(self, sql: str) -> tuple[bool, str]:
        """
        Basic SQL validation
        åŸºæœ¬çš„SQLéªŒè¯
        """
        if not sql or not sql.strip():
            return False, "SQLæŸ¥è¯¢ä¸ºç©º"
        
        sql_upper = sql.upper().strip()
        
        # Check for dangerous operations (basic security)
        dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER', 'CREATE', 'INSERT', 'UPDATE']
        for keyword in dangerous_keywords:
            if keyword in sql_upper and not sql_upper.startswith('SELECT'):
                return False, f"ä¸å…è®¸æ‰§è¡Œ {keyword} æ“ä½œ"
        
        # Check for basic SQL structure
        if not sql_upper.startswith('SELECT') and not sql_upper.startswith('WITH'):
            return False, "åªæ”¯æŒ SELECT æŸ¥è¯¢"
        
        return True, ""

    def parse_view_response(self, speak, data, prompt_response=None, mode="simple"):
        """
        Parse view response with dual-mode output support
        è§£æè§†å›¾å“åº”ï¼Œæ”¯æŒåŒæ¨¡å¼è¾“å‡º
        
        Args:
            speak: AI response text
            data: Query result data or callable
            prompt_response: Parsed prompt response (optional)
            mode: Output mode - "simple" (default, Markdown format) or "enhanced" (chart-view format)
        """
        logger.info(f"DEBUG parse_view_response called with mode: {mode}")
        logger.info(f"DEBUG parse_view_response called with speak: {speak}")
        logger.info(f"DEBUG parse_view_response called with data type: {type(data)}")
        logger.info(f"DEBUG parse_view_response called with prompt_response type: {type(prompt_response)}")
        
        if hasattr(prompt_response, 'direct_response'):
            logger.info(f"DEBUG prompt_response.direct_response: {prompt_response.direct_response}")
        if hasattr(prompt_response, 'sql'):
            logger.info(f"DEBUG prompt_response.sql: {prompt_response.sql}")
        
        try:
            # Check if we have analysis report - if so, we should execute SQL and format the full report
            has_analysis_report = (hasattr(prompt_response, 'analysis_report') and
                                 prompt_response.analysis_report and
                                 isinstance(prompt_response.analysis_report, dict) and
                                 any(prompt_response.analysis_report.values()))

            # ğŸš¨ æ”¹è¿›ï¼šä¼˜å…ˆå¤„ç†æœ‰æ„ä¹‰çš„direct_responseï¼Œæ”¯æŒéSQLæŸ¥è¯¢
            if not hasattr(prompt_response, 'sql') or not prompt_response.sql:
                # å¦‚æœæœ‰direct_responseï¼Œä¼˜å…ˆè¿”å›å®ƒï¼ˆè¿™æ˜¯AIå¯¹æ¦‚å¿µæ€§é—®é¢˜çš„å›ç­”ï¼‰
                if (hasattr(prompt_response, 'direct_response') and
                    prompt_response.direct_response and
                    prompt_response.direct_response.strip()):

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„å›ç­”ï¼ˆä¸æ˜¯é”™è¯¯ä¿¡æ¯ï¼‰
                    direct_resp = prompt_response.direct_response.strip()

                    # å¦‚æœæ˜¯è¡¨ç»“æ„ä¸è¶³çš„æç¤ºï¼Œæä¾›æ›´å‹å¥½çš„å›å¤
                    if "è¡¨ç»“æ„ä¿¡æ¯ä¸è¶³" in direct_resp or "ä¸è¶³ä»¥ç”Ÿæˆ" in direct_resp:
                        formatted_response = f"""ğŸ’¬ **AIåˆ†æå›å¤**

{speak if speak else direct_resp}

ğŸ“‹ **è¯´æ˜**:
- å½“å‰æŸ¥è¯¢å¯èƒ½æ˜¯æ¦‚å¿µæ€§é—®é¢˜ï¼Œä¸éœ€è¦å…·ä½“çš„æ•°æ®åº“æŸ¥è¯¢
- æˆ–è€…éœ€è¦æ›´å¤šçš„è¡¨ç»“æ„ä¿¡æ¯æ‰èƒ½ç”Ÿæˆå‡†ç¡®çš„SQLæŸ¥è¯¢

ğŸ’¡ **å»ºè®®**:
- å¦‚æœæ‚¨éœ€è¦æŸ¥è¯¢å…·ä½“æ•°æ®ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„è¡¨åå’Œå­—æ®µä¿¡æ¯
- å¦‚æœè¿™æ˜¯æ¦‚å¿µæ€§é—®é¢˜ï¼Œä¸Šè¿°AIå›å¤å¯èƒ½å·²ç»åŒ…å«äº†æ‚¨éœ€è¦çš„ä¿¡æ¯
- æ‚¨å¯ä»¥å°è¯•è¯¢é—®ï¼š"æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„è¡¨"æˆ–"æŸ¥è¯¢customer_infoè¡¨çš„ç»“æ„\""""

                        logger.info(f"Returning formatted direct response for insufficient table info")
                        return formatted_response
                    else:
                        # å…¶ä»–ç±»å‹çš„direct_responseï¼Œç›´æ¥è¿”å›ä½†æ ¼å¼åŒ–
                        formatted_response = f"""ğŸ’¬ **AIå›å¤**

{direct_resp}

ğŸ“‹ **è¯´æ˜**: AIæ¨¡å‹åŸºäºæ‚¨çš„æŸ¥è¯¢æä¾›äº†æ¦‚å¿µæ€§å›ç­”

ğŸ’¡ **æç¤º**: å¦‚æœæ‚¨éœ€è¦æŸ¥è¯¢å…·ä½“æ•°æ®ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºéœ€è¦æŸ¥è¯¢çš„è¡¨åå’Œå­—æ®µ"""

                        logger.info(f"Returning formatted direct response")
                        return formatted_response

                # If we have analysis report but no SQL, format the report without data
                if has_analysis_report:
                    return self._format_analysis_report_only(prompt_response.analysis_report)

                # æœ€åçš„å…œåº•å¤„ç†ï¼šæä¾›æœ‰ç”¨çš„ä¿¡æ¯è€Œä¸æ˜¯é”™è¯¯
                fallback_msg = f"""ğŸ“‹ **æŸ¥è¯¢å¤„ç†ç»“æœ**

ğŸ¤– **AIåˆ†æ**: {speak if speak else "AIæ¨¡å‹å·²å¤„ç†æ‚¨çš„æŸ¥è¯¢"}

ğŸ’¡ **è¯´æ˜**:
- æ‚¨çš„æŸ¥è¯¢å¯èƒ½æ˜¯æ¦‚å¿µæ€§é—®é¢˜ï¼Œä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢
- æˆ–è€…å½“å‰æ•°æ®åº“ä¿¡æ¯ä¸è¶³ä»¥ç”Ÿæˆå…·ä½“çš„SQLæŸ¥è¯¢

ğŸ”§ **å»ºè®®**:
- å¦‚æœéœ€è¦æŸ¥è¯¢æ•°æ®ï¼Œè¯·æä¾›å…·ä½“çš„è¡¨åå’Œå­—æ®µå
- å°è¯•è¯¢é—®ï¼š"æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨"
- æˆ–è€…é‡æ–°æè¿°æ‚¨çš„æ•°æ®æŸ¥è¯¢éœ€æ±‚"""

                logger.info(f"No SQL generated, returning fallback informative message")
                return fallback_msg
            
            original_sql = prompt_response.sql.strip()
            logger.info(f"DEBUG Original SQL: {original_sql}")
            
            # Apply SQL fixes
            fixed_sql, fixes_applied = self.sql_fixer.fix_sql(original_sql)
            
            if fixes_applied:
                logger.info(f"Applied SQL fixes: {fixes_applied}")
                sql_to_execute = fixed_sql
                fix_info = f"\nğŸ”§ è‡ªåŠ¨ä¿®å¤: {', '.join(fixes_applied)}"
            else:
                sql_to_execute = original_sql
                fix_info = ""
            
            logger.info(f"DEBUG SQL to execute: {sql_to_execute}")
            
            # Basic SQL validation
            is_valid, validation_error = self.validate_sql_basic(sql_to_execute)
            if not is_valid:
                # ğŸš¨ æ”¹è¿›ï¼šSQLéªŒè¯å¤±è´¥æ—¶å±•ç¤ºå®Œæ•´ä¿¡æ¯
                error_response = f"""ğŸ“‹ **SQLéªŒè¯å¤±è´¥**

ğŸ” **éªŒè¯é”™è¯¯**: {validation_error}

ğŸ“ **åŸå§‹SQL**:
```sql
{original_sql}
```"""
                
                if fixes_applied:
                    error_response += f"""

ğŸ”§ **ä¿®å¤åçš„SQL**:
```sql
{sql_to_execute}
```

âš™ï¸ **åº”ç”¨çš„ä¿®å¤**: {', '.join(fixes_applied)}"""
                
                error_response += """

ğŸ’¡ **å»ºè®®**: 
- è¯·æ£€æŸ¥SQLè¯­æ³•æ˜¯å¦æ­£ç¡®
- ç¡®è®¤è¡¨åå’Œå­—æ®µåæ˜¯å¦å­˜åœ¨
- é¿å…ä½¿ç”¨å±é™©çš„SQLæ“ä½œï¼ˆå¦‚DROPã€DELETEç­‰ï¼‰"""
                
                logger.error(f"SQL validation failed: {validation_error}")
                return error_response
            
            # Execute SQL with enhanced error handling
            try:
                result = data(sql_to_execute)
                
                if result is None or result.empty:
                    # Even with empty results, show SQL and analysis report if available
                    if mode == "simple":
                        # Simple mode: Return Markdown format
                        empty_result_msg = f"""ğŸ“Š **æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸ**

âœ… **SQLæ‰§è¡ŒçŠ¶æ€**: æˆåŠŸæ‰§è¡Œï¼Œä½†æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®

ğŸ’¡ **å»ºè®®**: è¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶æˆ–æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨

"""
                        
                        # Add SQL section
                        empty_result_msg += "="*60 + "\n"
                        empty_result_msg += "ğŸ”§ **æ‰§è¡Œçš„SQLæŸ¥è¯¢**\n"
                        empty_result_msg += "="*60 + "\n\n"
                        empty_result_msg += "```sql\n"
                        empty_result_msg += sql_to_execute.strip()
                        empty_result_msg += "\n```\n\n"
                        empty_result_msg += "ğŸ’¡ **SQLè¯´æ˜**: ä»¥ä¸Šæ˜¯æ‰§è¡Œçš„SQLè¯­å¥ï¼Œè™½ç„¶æ²¡æœ‰è¿”å›æ•°æ®ï¼Œä½†SQLæ‰§è¡ŒæˆåŠŸ\n"
                        
                        # Add analysis report if available
                        if has_analysis_report:
                            empty_result_msg += "\n\n" + "="*60 + "\n"
                            empty_result_msg += "ğŸ“‹ **åˆ†ææŠ¥å‘Š** (åŸºäºæŸ¥è¯¢æ„å›¾)\n"
                            empty_result_msg += "="*60 + "\n\n"
                            empty_result_msg += self._format_analysis_report_only(prompt_response.analysis_report)
                        
                        return empty_result_msg + fix_info
                    else:
                        # Enhanced mode: Generate chart-view format
                        return self._generate_chart_view_format(result, sql_to_execute, prompt_response, fix_info)
                
                # Format result for display based on mode
                if mode == "simple":
                    # Simple mode: Return Markdown format (default)
                    view_content = self._format_result_for_display(result, prompt_response)
                    return view_content + fix_info
                else:
                    # Enhanced mode: Generate chart-view format for frontend rendering
                    return self._generate_chart_view_format(result, sql_to_execute, prompt_response, fix_info)
                
            except (SQLAlchemyError, pymysql.Error, Exception) as sql_error:
                # If fixed SQL still fails, try the original SQL
                if fixes_applied:
                    logger.info("Fixed SQL failed, trying original SQL...")
                    try:
                        result = data(original_sql)
                        if result is not None and not result.empty:
                            if mode == "simple":
                                view_content = self._format_result_for_display(result, prompt_response)
                                return view_content + "\nâš ï¸ æ³¨æ„: ä½¿ç”¨äº†åŸå§‹SQLæŸ¥è¯¢ï¼ˆè‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼‰"
                            else:
                                return self._generate_chart_view_format(result, original_sql, prompt_response, "\nâš ï¸ æ³¨æ„: ä½¿ç”¨äº†åŸå§‹SQLæŸ¥è¯¢ï¼ˆè‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼‰")
                    except Exception as fallback_error:
                        logger.info(f"Original SQL also failed: {fallback_error}")
                        # Continue with comprehensive error handling below
                
                # ğŸš¨ æ”¹è¿›ï¼šæä¾›æœ€è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬SQLå†…å®¹
                user_friendly_error = self.format_sql_error_for_user(sql_error, sql_to_execute)
                technical_error = str(sql_error)
                
                logger.error(f"SQL execution failed: {technical_error}")
                logger.error(f"SQL that failed: {sql_to_execute}")
                
                # Return comprehensive error information with SQL display
                error_response = f"""ğŸ“‹ **æ•°æ®åº“æŸ¥è¯¢è¯¦ç»†ä¿¡æ¯**

âŒ **æ‰§è¡ŒçŠ¶æ€**: æŸ¥è¯¢å¤±è´¥

ğŸ” **é”™è¯¯åŸå› **: {user_friendly_error}

ğŸ“ **æ‰§è¡Œçš„SQL**:
```sql
{sql_to_execute}
```"""

                # Show original SQL if it was modified
                if fixes_applied and sql_to_execute != original_sql:
                    error_response += f"""

ğŸ“ **åŸå§‹SQL**:
```sql
{original_sql}
```

ğŸ”§ **å·²å°è¯•çš„ä¿®å¤**: {', '.join(fixes_applied)}"""

                error_response += f"""

ğŸ”§ **æŠ€æœ¯è¯¦æƒ…**: {technical_error}

ğŸ’¡ **å»ºè®®**: 
- æ£€æŸ¥è¡¨åå’Œå­—æ®µåæ˜¯å¦æ­£ç¡®
- ç¡®è®¤æ•°æ®åº“ä¸­æ˜¯å¦å­˜åœ¨ç›¸å…³æ•°æ®
- å°è¯•ç®€åŒ–æŸ¥è¯¢æ¡ä»¶
- æ£€æŸ¥SQLè¯­æ³•æ˜¯å¦ç¬¦åˆMySQLæ ‡å‡†"""

                # Add analysis report if available, even when SQL fails
                if has_analysis_report:
                    error_response += "\n\n" + "="*60 + "\n"
                    error_response += "ğŸ“‹ **AIåˆ†ææŠ¥å‘Š** (åŸºäºæŸ¥è¯¢æ„å›¾)\n"
                    error_response += "="*60 + "\n\n"
                    error_response += self._format_analysis_report_only(prompt_response.analysis_report)
                
                return error_response + fix_info
                
        except Exception as e:
            # ğŸš¨ æ”¹è¿›ï¼šæœ€åçš„å…œåº•å¤„ç†ï¼Œç¡®ä¿æ°¸è¿œä¸æ˜¾ç¤ºé€šç”¨é”™è¯¯
            logger.error(f"Unexpected error in parse_view_response: {str(e)}")
            
            # Try to extract SQL from prompt_response if available
            sql_info = ""
            if hasattr(prompt_response, 'sql') and prompt_response.sql:
                sql_info = f"""

ğŸ“ **ç›¸å…³SQL**:
```sql
{prompt_response.sql}
```"""
            
            # Try to include AI response if available
            ai_response_info = ""
            if speak:
                ai_response_info = f"""

ğŸ’¬ **AIå›å¤å†…å®¹**:
```
{speak}
```"""
            
            return f"""ğŸ“‹ **ç³»ç»Ÿå¤„ç†ä¿¡æ¯**

âš ï¸ **å¤„ç†çŠ¶æ€**: ç³»ç»Ÿåœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†æ„å¤–æƒ…å†µ

ğŸ”§ **æŠ€æœ¯è¯¦æƒ…**: {str(e)}{sql_info}{ai_response_info}

ğŸ’¡ **å»ºè®®**: 
- è¯·å°è¯•é‡æ–°æäº¤æ‚¨çš„æŸ¥è¯¢
- å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·ç®€åŒ–æ‚¨çš„æŸ¥è¯¢æ¡ä»¶
- æ‚¨å¯ä»¥å°è¯•åˆ†æ­¥éª¤æŸ¥è¯¢æ¥å®šä½é—®é¢˜"""

    def _format_dataframe_as_markdown_table(self, df):
        """
        Convert DataFrame to a well-formatted Markdown table
        å°†DataFrameè½¬æ¢ä¸ºæ ¼å¼è‰¯å¥½çš„Markdownè¡¨æ ¼
        """
        try:
            if df.empty:
                return "ğŸ“Š æŸ¥è¯¢ç»“æœä¸ºç©º"
            
            # Get column names
            columns = list(df.columns)
            
            # Create table header
            header = "| " + " | ".join(columns) + " |"
            separator = "|" + "|".join([" --- " for _ in columns]) + "|"
            
            # Create data rows
            rows = []
            for _, row in df.iterrows():
                formatted_row = []
                for col in columns:
                    value = row[col]
                    # Format different types of values
                    if pd.isna(value) or value is None:
                        formatted_row.append("-")
                    elif isinstance(value, (int, float)):
                        if col.upper().startswith('MOB') or 'ç‡' in col or 'rate' in col.lower():
                            # Format as percentage for rate columns
                            if value == 0:
                                formatted_row.append("0.00%")
                            else:
                                formatted_row.append(f"{value:.2%}")
                        else:
                            # Format as regular number
                            formatted_row.append(f"{value:,.2f}")
                    else:
                        # Format as string
                        formatted_row.append(str(value))
                rows.append("| " + " | ".join(formatted_row) + " |")
            
            # Combine all parts
            table = "\n".join([header, separator] + rows)
            return table
            
        except Exception as e:
            logger.error(f"Error creating Markdown table: {str(e)}")
            # Fallback to original format
            return df.to_string(index=False, max_rows=50)

    def _format_result_for_display(self, result, prompt_response):
        """
        Format query result for display with analysis report and SQL
        æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœç”¨äºæ˜¾ç¤ºï¼ŒåŒ…å«åˆ†ææŠ¥å‘Šå’ŒSQL
        """
        try:
            # Convert DataFrame to a user-friendly format
            if len(result) == 0:
                return "ğŸ“Š æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®ã€‚"
            
            # Handle duplicate columns in DataFrame
            if hasattr(result, 'columns'):
                original_columns = list(result.columns)
                if len(original_columns) != len(set(original_columns)):
                    logger.info("æ£€æµ‹åˆ°DataFrameé‡å¤åˆ—åï¼Œæ­£åœ¨ä¿®å¤...")
                    
                    # Create new column names for duplicates
                    new_columns = []
                    column_counts = {}
                    
                    for col in original_columns:
                        if col in column_counts:
                            column_counts[col] += 1
                            new_col_name = f"{col}_{column_counts[col]}"
                        else:
                            column_counts[col] = 0
                            new_col_name = col
                        new_columns.append(new_col_name)
                    
                    # Apply new column names
                    result.columns = new_columns
                    logger.info(f"DataFrameåˆ—åå·²ä¿®å¤: {original_columns} -> {new_columns}")
            
            # Create a formatted table using Markdown format
            formatted_result = "ğŸ“Š **æŸ¥è¯¢ç»“æœ**\n\n"
            
            # Add table description if it looks like a pivot table
            if any('MOB' in str(col) for col in result.columns):
                formatted_result += "**é€¾æœŸç‡åˆ†æè¡¨** (æŒ‰æ”¾æ¬¾æœˆä»½å’ŒMOBæœŸæ•°)\n\n"
            
            # Use Markdown table format for better readability
            markdown_table = self._format_dataframe_as_markdown_table(result)
            formatted_result += markdown_table
            
            # Add record count info
            if len(result) > 50:
                formatted_result += f"\n\nğŸ“‹ æ˜¾ç¤ºå‰50æ¡è®°å½•ï¼Œå…± **{len(result)}** æ¡è®°å½•"
            else:
                formatted_result += f"\n\nğŸ“‹ å…± **{len(result)}** æ¡è®°å½•"
            
            # Add data interpretation for rate tables
            if any('MOB' in str(col) for col in result.columns):
                formatted_result += "\n\nğŸ’¡ **æ•°æ®è¯´æ˜**:\n"
                formatted_result += "- MOB (Months on Books): æ”¾æ¬¾åçš„æœˆæ•°\n"
                formatted_result += "- é€¾æœŸç‡ä»¥ç™¾åˆ†æ¯”æ˜¾ç¤ºï¼Œ'-' è¡¨ç¤ºæš‚æ— æ•°æ®\n"
                formatted_result += "- æ•°æ®æŒ‰æ”¾æ¬¾æœˆä»½æ’åˆ—ï¼Œä¾¿äºè¶‹åŠ¿åˆ†æ\n"
            
            # Add SQL section if available
            if hasattr(prompt_response, 'sql') and prompt_response.sql:
                formatted_result += "\n\n" + "="*60 + "\n"
                formatted_result += "ğŸ”§ **æ‰§è¡Œçš„SQLæŸ¥è¯¢**\n"
                formatted_result += "="*60 + "\n\n"
                formatted_result += "```sql\n"
                formatted_result += prompt_response.sql.strip()
                formatted_result += "\n```\n\n"
                formatted_result += "ğŸ’¡ **SQLè¯´æ˜**: ä»¥ä¸Šæ˜¯ç”Ÿæˆæ­¤æŸ¥è¯¢ç»“æœçš„SQLè¯­å¥ï¼Œæ‚¨å¯ä»¥å‚è€ƒæˆ–å¤åˆ¶ä½¿ç”¨\n"
            
            # Add analysis report if available
            if hasattr(prompt_response, 'analysis_report') and prompt_response.analysis_report:
                report = prompt_response.analysis_report
                formatted_result += "\n\n" + "="*60 + "\n"
                formatted_result += "ğŸ“‹ **åˆ†ææŠ¥å‘Š**\n"
                formatted_result += "="*60 + "\n\n"
                
                if report.get('summary'):
                    formatted_result += f"**ğŸ“ åˆ†ææ‘˜è¦:**\n{report['summary']}\n\n"
                
                if report.get('key_findings') and isinstance(report['key_findings'], list):
                    formatted_result += "**ğŸ” å…³é”®å‘ç°:**\n"
                    for i, finding in enumerate(report['key_findings'], 1):
                        formatted_result += f"{i}. {finding}\n"
                    formatted_result += "\n"
                
                if report.get('insights') and isinstance(report['insights'], list):
                    formatted_result += "**ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿ:**\n"
                    for i, insight in enumerate(report['insights'], 1):
                        formatted_result += f"{i}. {insight}\n"
                    formatted_result += "\n"
                
                if report.get('recommendations') and isinstance(report['recommendations'], list):
                    formatted_result += "**ğŸ¯ å»ºè®®æªæ–½:**\n"
                    for i, rec in enumerate(report['recommendations'], 1):
                        formatted_result += f"{i}. {rec}\n"
                    formatted_result += "\n"
                
                if report.get('methodology'):
                    formatted_result += f"**ğŸ”¬ åˆ†ææ–¹æ³•:**\n{report['methodology']}\n\n"
            
            return formatted_result
            
        except Exception as e:
            logger.error(f"Error formatting result: {str(e)}")
            return f"âœ… æŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œä½†ç»“æœæ ¼å¼åŒ–å¤±è´¥: {str(e)}"

    def _format_analysis_report_only(self, analysis_report):
        """
        Format analysis report without query data
        ä»…æ ¼å¼åŒ–åˆ†ææŠ¥å‘Šï¼ˆæ— æŸ¥è¯¢æ•°æ®ï¼‰
        """
        try:
            formatted_result = "ğŸ“‹ **åˆ†ææŠ¥å‘Š**\n"
            formatted_result += "="*60 + "\n\n"
            
            if analysis_report.get('summary'):
                formatted_result += f"**ğŸ“ åˆ†ææ‘˜è¦:**\n{analysis_report['summary']}\n\n"
            
            if analysis_report.get('key_findings') and isinstance(analysis_report['key_findings'], list):
                formatted_result += "**ğŸ” å…³é”®å‘ç°:**\n"
                for i, finding in enumerate(analysis_report['key_findings'], 1):
                    formatted_result += f"{i}. {finding}\n"
                formatted_result += "\n"
            
            if analysis_report.get('insights') and isinstance(analysis_report['insights'], list):
                formatted_result += "**ğŸ’¡ ä¸šåŠ¡æ´å¯Ÿ:**\n"
                for i, insight in enumerate(analysis_report['insights'], 1):
                    formatted_result += f"{i}. {insight}\n"
                formatted_result += "\n"
            
            if analysis_report.get('recommendations') and isinstance(analysis_report['recommendations'], list):
                formatted_result += "**ğŸ¯ å»ºè®®æªæ–½:**\n"
                for i, rec in enumerate(analysis_report['recommendations'], 1):
                    formatted_result += f"{i}. {rec}\n"
                formatted_result += "\n"
            
            if analysis_report.get('methodology'):
                formatted_result += f"**ğŸ”¬ åˆ†ææ–¹æ³•:**\n{analysis_report['methodology']}\n\n"
            
            return formatted_result
            
        except Exception as e:
            logger.error(f"Error formatting analysis report: {str(e)}")
            return f"ğŸ“‹ åˆ†ææŠ¥å‘Šæ ¼å¼åŒ–å¤±è´¥: {str(e)}"

    def _generate_chart_view_format(self, result, sql, prompt_response, fix_info):
        """
        Generate chart-view format for frontend rendering
        ç”Ÿæˆchart-viewæ ¼å¼ç”¨äºå‰ç«¯æ¸²æŸ“
        
        Args:
            result: Query result DataFrame
            sql: SQL query string
            prompt_response: Parsed prompt response
            fix_info: SQL fix information
            
        Returns:
            str: chart-view formatted string
        """
        try:
            import json
            import xml.etree.ElementTree as ET
            from dbgpt.util.json_utils import serialize
            
            # Prepare chart-view data
            param = {}
            param["type"] = "response_table"
            param["sql"] = sql
            
            if result is not None and not result.empty:
                # Convert DataFrame to JSON format
                param["data"] = json.loads(
                    result.to_json(orient="records", date_format="iso", date_unit="s")
                )
            else:
                param["data"] = []
            
            # Add analysis report if available
            if (hasattr(prompt_response, 'analysis_report') and 
                prompt_response.analysis_report and 
                isinstance(prompt_response.analysis_report, dict)):
                param["analysis_report"] = prompt_response.analysis_report
            
            # Generate chart-view XML element
            view_json_str = json.dumps(param, default=serialize, ensure_ascii=False)
            api_call_element = ET.Element("chart-view")
            api_call_element.set("content", view_json_str)
            result_xml = ET.tostring(api_call_element, encoding="utf-8")
            
            chart_view_content = result_xml.decode("utf-8")
            
            # Add fix info if available
            if fix_info:
                chart_view_content += fix_info
                
            return chart_view_content
            
        except Exception as e:
            logger.error(f"Error generating chart-view format: {str(e)}")
            # Fallback to simple mode if chart-view generation fails
            return self._format_result_for_display(result, prompt_response) + fix_info
