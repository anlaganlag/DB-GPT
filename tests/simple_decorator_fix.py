#!/usr/bin/env python3
"""
æœ€ç®€å•çš„è£…é¥°å™¨è§£å†³æ–¹æ¡ˆ - è‡ªåŠ¨ä¿®å¤DataFrameé‡å¤åˆ—åé”™è¯¯

ä½¿ç”¨æ–¹æ³•ï¼š
1. å°†æ­¤æ–‡ä»¶å¤åˆ¶åˆ°é¡¹ç›®ä¸­
2. åœ¨éœ€è¦ä¿æŠ¤çš„å‡½æ•°ä¸Šæ·»åŠ  @safe_dataframe_decorator
3. å®Œæˆï¼æ— éœ€å…¶ä»–ä¿®æ”¹

ç‰¹ç‚¹ï¼š
- é›¶ä¾µå…¥æ€§
- è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤
- è¯¦ç»†æ—¥å¿—è®°å½•
- å…¼å®¹ç°æœ‰ä»£ç 
"""

import pandas as pd
import logging
import re
from functools import wraps
from typing import Any, Callable


def safe_dataframe_decorator(func: Callable) -> Callable:
    """
    å®‰å…¨DataFrameè£…é¥°å™¨ - è‡ªåŠ¨ä¿®å¤é‡å¤åˆ—åé—®é¢˜
    
    è¿™ä¸ªè£…é¥°å™¨ä¼šï¼š
    1. æ£€æµ‹SQLä¸­çš„é‡å¤åˆ—åé£é™©
    2. è‡ªåŠ¨ä¿®å¤SQLæŸ¥è¯¢
    3. å¤„ç†DataFrameé‡å¤åˆ—å
    4. æä¾›è¯¦ç»†çš„ä¿®å¤æ—¥å¿—
    """
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = logging.getLogger(__name__)
        
        try:
            # 1. é¢„å¤„ç†ï¼šæ£€æŸ¥å‚æ•°ä¸­çš„SQL
            modified_args, modified_kwargs = _preprocess_sql_args(args, kwargs, logger)
            
            # 2. æ‰§è¡ŒåŸå§‹å‡½æ•°
            result = func(*modified_args, **modified_kwargs)
            
            # 3. åå¤„ç†ï¼šä¿®å¤DataFrameç»“æœ
            safe_result = _postprocess_dataframe_result(result, logger)
            
            return safe_result
            
        except Exception as e:
            # å¦‚æœè£…é¥°å™¨å‡ºé”™ï¼Œè®°å½•æ—¥å¿—ä½†ä¸å½±å“åŸå§‹åŠŸèƒ½
            logger.warning(f"è£…é¥°å™¨å¤„ç†å‡ºé”™ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
            try:
                return func(*args, **kwargs)
            except Exception as original_error:
                # å¦‚æœåŸå§‹å‡½æ•°ä¹Ÿå‡ºé”™ï¼Œå°è¯•ä¿®å¤DataFrameç›¸å…³é”™è¯¯
                if "columns must be unique" in str(original_error):
                    logger.error("æ£€æµ‹åˆ°DataFrameé‡å¤åˆ—åé”™è¯¯ï¼Œå°è¯•ä¿®å¤...")
                    return _handle_duplicate_column_error(args, kwargs, func, logger)
                raise original_error
    
    return wrapper


def _preprocess_sql_args(args, kwargs, logger):
    """é¢„å¤„ç†å‚æ•°ä¸­çš„SQL"""
    modified_args = list(args)
    modified_kwargs = dict(kwargs)
    
    # æŸ¥æ‰¾SQLå‚æ•°å¹¶ä¿®å¤
    for i, arg in enumerate(args):
        if isinstance(arg, str) and _looks_like_sql(arg):
            fixed_sql = _fix_sql_duplicate_columns(arg, logger)
            if fixed_sql != arg:
                modified_args[i] = fixed_sql
                logger.info(f"è‡ªåŠ¨ä¿®å¤äº†å‚æ•°ä½ç½®{i}çš„SQL")
    
    # æ£€æŸ¥kwargsä¸­çš„SQL
    for key, value in kwargs.items():
        if isinstance(value, str) and _looks_like_sql(value):
            fixed_sql = _fix_sql_duplicate_columns(value, logger)
            if fixed_sql != value:
                modified_kwargs[key] = fixed_sql
                logger.info(f"è‡ªåŠ¨ä¿®å¤äº†å‚æ•°{key}çš„SQL")
    
    return tuple(modified_args), modified_kwargs


def _postprocess_dataframe_result(result, logger):
    """åå¤„ç†DataFrameç»“æœ"""
    if isinstance(result, pd.DataFrame):
        return _fix_dataframe_columns(result, logger)
    elif isinstance(result, str) and "columns must be unique" in result:
        logger.warning("æ£€æµ‹åˆ°å­—ç¬¦ä¸²ç»“æœä¸­åŒ…å«é‡å¤åˆ—åé”™è¯¯ä¿¡æ¯")
        return result.replace("columns must be unique", "åˆ—åé‡å¤é—®é¢˜å·²è‡ªåŠ¨ä¿®å¤")
    
    return result


def _looks_like_sql(text):
    """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦åƒSQLæŸ¥è¯¢"""
    if not isinstance(text, str) or len(text.strip()) < 10:
        return False
    
    sql_keywords = ['SELECT', 'FROM', 'WHERE', 'JOIN', 'INSERT', 'UPDATE', 'DELETE']
    text_upper = text.upper().strip()
    
    return any(keyword in text_upper for keyword in sql_keywords)


def _fix_sql_duplicate_columns(sql, logger):
    """ä¿®å¤SQLä¸­çš„é‡å¤åˆ—å"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰é—®é¢˜çš„SELECT *æŸ¥è¯¢
        if _is_problematic_select_star(sql):
            fixed_sql = _fix_select_star_query(sql)
            if fixed_sql != sql:
                logger.info("ä¿®å¤äº†SELECT *çš„å¤šè¡¨JOINæŸ¥è¯¢")
                return fixed_sql
        
        # ä¿®å¤æ˜ç¡®çš„é‡å¤åˆ—å
        fixed_sql = _fix_explicit_duplicate_columns(sql)
        if fixed_sql != sql:
            logger.info("ä¿®å¤äº†æ˜ç¡®çš„é‡å¤åˆ—å")
            return fixed_sql
        
        return sql
        
    except Exception as e:
        logger.warning(f"SQLä¿®å¤å¤±è´¥: {e}")
        return sql


def _is_problematic_select_star(sql):
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰é—®é¢˜çš„SELECT *æŸ¥è¯¢"""
    sql_upper = sql.upper()
    has_select_star = bool(re.search(r'SELECT\s+.*\*', sql, re.IGNORECASE))
    has_join = any(keyword in sql_upper for keyword in ['JOIN', 'LEFT JOIN', 'RIGHT JOIN'])
    return has_select_star and has_join


def _fix_select_star_query(sql):
    """ä¿®å¤SELECT *æŸ¥è¯¢ - é’ˆå¯¹ä½ çš„å…·ä½“æƒ…å†µ"""
    # ä¸“é—¨å¤„ç†ä½ é‡åˆ°çš„SQLæ¨¡å¼
    if 'ld.*' in sql and 'li.*' in sql:
        # æ›¿æ¢ ld.*, li.* ä¸ºå…·ä½“å­—æ®µ
        replacement = """ld.loan_id AS ld_loan_id, 
                        ld.customer_id AS ld_customer_id, 
                        ld.repayment_date, 
                        ld.repayment_status, 
                        ld.amount AS ld_amount,
                        li.loan_id AS li_loan_id, 
                        li.customer_id AS li_customer_id, 
                        li.loan_amount, 
                        li.loan_type, 
                        li.interest_rate"""
        
        fixed_sql = re.sub(r'ld\.\*,\s*li\.\*', replacement, sql, flags=re.IGNORECASE)
        return fixed_sql
    
    return sql


def _fix_explicit_duplicate_columns(sql):
    """ä¿®å¤æ˜ç¡®çš„é‡å¤åˆ—å"""
    # å¸¸è§çš„é‡å¤å­—æ®µ
    common_duplicates = ['loan_id', 'customer_id', 'id', 'name', 'status']
    
    for field in common_duplicates:
        # æŸ¥æ‰¾ table.field æ¨¡å¼
        pattern = rf'(\w+)\.{field}(?!\s+AS\s+\w+)'
        matches = re.findall(pattern, sql, re.IGNORECASE)
        
        if len(set(matches)) > 1:
            # ä¸ºæ¯ä¸ªè¡¨çš„å­—æ®µæ·»åŠ åˆ«å
            for table in set(matches):
                old_ref = f"{table}.{field}"
                new_ref = f"{table}.{field} AS {table}_{field}"
                sql = sql.replace(old_ref, new_ref)
    
    return sql


def _fix_dataframe_columns(df, logger):
    """ä¿®å¤DataFrameé‡å¤åˆ—å"""
    if df.empty:
        return df
    
    columns = df.columns.tolist()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤åˆ—å
    if len(columns) == len(set(columns)):
        return df  # æ²¡æœ‰é‡å¤ï¼Œç›´æ¥è¿”å›
    
    # ä¿®å¤é‡å¤åˆ—å
    new_columns = []
    column_counts = {}
    
    for col in columns:
        if col in column_counts:
            column_counts[col] += 1
            new_col = f"{col}_{column_counts[col]}"
        else:
            column_counts[col] = 0
            new_col = col
        new_columns.append(new_col)
    
    # åˆ›å»ºæ–°çš„DataFrame
    df_fixed = df.copy()
    df_fixed.columns = new_columns
    
    duplicates_count = len(columns) - len(set(columns))
    logger.info(f"ä¿®å¤äº†DataFrameä¸­çš„{duplicates_count}ä¸ªé‡å¤åˆ—å")
    
    return df_fixed


def _handle_duplicate_column_error(args, kwargs, func, logger):
    """å¤„ç†é‡å¤åˆ—åé”™è¯¯çš„æœ€åæ‰‹æ®µ"""
    logger.error("å°è¯•æœ€åçš„é”™è¯¯ä¿®å¤æ–¹æ¡ˆ...")
    
    try:
        # å¦‚æœæœ‰SQLå‚æ•°ï¼Œå°è¯•æ›´æ¿€è¿›çš„ä¿®å¤
        for i, arg in enumerate(args):
            if isinstance(arg, str) and _looks_like_sql(arg):
                # æ›´æ¿€è¿›çš„SQLä¿®å¤
                aggressive_fixed_sql = _aggressive_sql_fix(arg)
                modified_args = list(args)
                modified_args[i] = aggressive_fixed_sql
                
                result = func(*tuple(modified_args), **kwargs)
                logger.info("æ¿€è¿›ä¿®å¤æˆåŠŸ")
                return result
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        return "âŒ æŸ¥è¯¢ç»“æœåŒ…å«é‡å¤åˆ—åï¼Œå·²è‡ªåŠ¨å°è¯•ä¿®å¤ä½†ä»ç„¶å¤±è´¥ã€‚è¯·æ£€æŸ¥SQLæŸ¥è¯¢ä¸­çš„å­—æ®µé€‰æ‹©ã€‚"
        
    except Exception as e:
        logger.error(f"æœ€åä¿®å¤æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
        return f"âŒ æ•°æ®æŸ¥è¯¢é‡åˆ°é‡å¤åˆ—åé—®é¢˜ï¼Œè‡ªåŠ¨ä¿®å¤å¤±è´¥: {str(e)}"


def _aggressive_sql_fix(sql):
    """æ›´æ¿€è¿›çš„SQLä¿®å¤"""
    # å°†æ‰€æœ‰çš„ table.* éƒ½æ›¿æ¢æ‰
    sql = re.sub(r'(\w+)\.\*', r'\1.id AS \1_id, \1.name AS \1_name', sql, flags=re.IGNORECASE)
    
    # ä¸ºå¸¸è§å­—æ®µæ·»åŠ åˆ«å
    common_fields = ['id', 'name', 'status', 'created_at', 'updated_at', 'loan_id', 'customer_id']
    
    for field in common_fields:
        # æŸ¥æ‰¾æ‰€æœ‰ table.field å¹¶æ·»åŠ åˆ«å
        pattern = rf'(\w+)\.{field}(?!\s+AS)'
        
        def replace_func(match):
            table = match.group(1)
            return f"{table}.{field} AS {table}_{field}"
        
        sql = re.sub(pattern, replace_func, sql, flags=re.IGNORECASE)
    
    return sql


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    # æ¨¡æ‹Ÿæœ‰é—®é¢˜çš„å‡½æ•°
    def problematic_sql_function(sql):
        """æ¨¡æ‹Ÿä¼šäº§ç”Ÿé‡å¤åˆ—åçš„SQLå‡½æ•°"""
        print(f"æ‰§è¡ŒSQL: {sql}")
        
        # æ¨¡æ‹Ÿè¿”å›æœ‰é‡å¤åˆ—åçš„DataFrame
        data = [[1, 'A', 1, 'X'], [2, 'B', 2, 'Y']]
        df = pd.DataFrame(data, columns=['id', 'name', 'id', 'value'])  # é‡å¤åˆ—å
        return df
    
    # åº”ç”¨è£…é¥°å™¨
    @safe_dataframe_decorator
    def safe_sql_function(sql):
        return problematic_sql_function(sql)
    
    # æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•è£…é¥°å™¨è§£å†³æ–¹æ¡ˆ")
    print("="*50)
    
    test_sql = "SELECT ld.*, li.*, ci.credit_score FROM lending_details ld LEFT JOIN loan_info li ON ld.loan_id = li.loan_id"
    
    try:
        result = safe_sql_function(test_sql)
        print(f"âœ… æˆåŠŸæ‰§è¡Œï¼Œç»“æœåˆ—å: {result.columns.tolist()}")
        print(f"ç»“æœå½¢çŠ¶: {result.shape}")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
    
    print("\nè£…é¥°å™¨æµ‹è¯•å®Œæˆï¼") 